{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習に関するテクニック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-db87a446c3cb>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-db87a446c3cb>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    from common import\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append('deep-learning-from-scratch')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from common import \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメータの更新\n",
    "\n",
    "### 冒険家の例え話 (の解釈)\n",
    "\n",
    "広い荒野の中で一番深い谷底を目指す冒険家。  \n",
    "しかし彼は目が見えない。  \n",
    "どうやったら彼は効率的に深い谷底へたどり着くことができるだろうか。 (BGM: 広野を行く)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SGD (確率的勾配降下法 stochastic gradient descent)\n",
    "\n",
    "重みパラメータWを次に `W - 学習率*δL/δW`に更新していく。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, param, grads):\n",
    "        for key in params.keys():\n",
    "            param[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr: learning rate.\n",
    "\n",
    "#### SGDの欠点\n",
    "関数の形状が等方的でなく、伸びた関数だとジグザグに学習が進み、とても非効率な探索をする傾向にある。\n",
    "\n",
    "TODO:// 実際に描画してみたい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "\n",
    "モーメンタム: 運動量\n",
    "\n",
    "- v_n+1 = αv_n - η(δL/δW)\n",
    "- W_n+1 = W_n + v_n+1\n",
    "\n",
    "\n",
    "各変数の役割\n",
    "\n",
    "- v: 速度\n",
    "- α: 徐々に減速させるための定数。0.9等の値を入れる。空気抵抗のようなもの。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momenmum\n",
    "        self.v =None\n",
    "    \n",
    "    def update(self, param, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "            param[key] += self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "NNでは学習係数ηが小さすぎると学習に時間がかかり、大きすぎると発散して正しい学習が行えない。\n",
    "学習係数に関する有効なテクニックとして、学習係数の減衰(learning rate decay)という方法が使われる。\n",
    "\n",
    "`AdaGrad`はパラメータの要素ごとに適応的に学習係数を調整しながら学習を行う手法。(Ada: Adaptive)\n",
    "\n",
    "- h_n+1 = h_n + ((δL/δW) ・(δL/δW))\n",
    "- W_n+1 = W_n - η(1/√h+1 * δL / δW)\n",
    "\n",
    "これまで経験した勾配の値を二乗和として保持し、よく動いた要素は学習係数が小さくなるような数式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h =None\n",
    "    \n",
    "    def update(self, param, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.h[key] = grads[key] * grads[key]\n",
    "            param[key] -= self.lr * grads[key] / (np.sqrt(self.h[key])) + 1e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "Momentum + AdaGrad\n",
    "効率的にパラメータ空間を探索できる。\n",
    "また、ハイパーパラメータの「バイアス補正」が行われていることも特徴\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### まとめ\n",
    "\n",
    "SGDよりも他の3手法のほうが早く学習が進む傾向があるが、どれが一番いいとは一概には言えない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重みの初期値の設定の仕方"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 隠れ層のアクティベーション分布\n",
    "\n",
    "隠れ層のアクティベーション(活性化関数の後のデータ)の分布が、重みの初期値によってどのように変化するかの実験を見ていく。\n",
    "\n",
    "アクティベーションに偏りがあると、表現力が制限されてしまう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavierの初期値\n",
    "\n",
    "データがn子あったときに、1 / √n の標準偏差を持つ分布。\n",
    "\n",
    "活性化関数が線形であることが前提。Sigmoid関数やTanhは左右対称で中央付近が線形関数としてみなすことが可能。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLUの場合の重み初期値: He\n",
    "\n",
    "前層のノード数がnの場合、√ n/2 を標準偏差とするガウス分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "2015年に提案された手法にも関わらず、現在多くの場所で使われている\n",
    "\n",
    "- 学習を早く進行させることができる\n",
    "- 初期値にあまり依存しない\n",
    "- 過学習を抑制する\n",
    "\n",
    "データ分布の正規化を行うBatchNormalizationレイヤを、NNに追加する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正則化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 過学習\n",
    "\n",
    "- パラメータを大量に持ち、表現力の高いモデル\n",
    "- 訓練データが少ない\n",
    "\n",
    "というような要件を満たすと、過学習が起こりやすい\n",
    "\n",
    "### Weight decay(荷重減衰)\n",
    "\n",
    "過学習を抑制する手法\n",
    "\n",
    "L2ノルムを加算する、Weight decay\n",
    "`(1/2)λW**2` (λが大きいほど、大きな値へのペナルティが大きくなる)\n",
    "\n",
    "W = (w1, w2, ...wn)があるとき L2ノルム = √(w1^2 + w2^2 + ... + wn^2)\n",
    "\n",
    "### dropout\n",
    "\n",
    "ニューロンをランダム消去しながら学習する。\n",
    "\n",
    "- 訓練時: 隠れ層のニューロンをランダムに選び出して、消去\n",
    "- テスト時: すべてのニューロンに信号を伝達するが、各ニューロンの出力に対して訓練時に消去した割合を乗算して出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.drop_ratio\n",
    "            return x * self.mask\n",
    "        return x * (1.0 - dropout_ratio)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ハイパーパラメータの検証と調整\n",
    "\n",
    "### ハイパーパラメータの最適化\n",
    "\n",
    "1. ハイパーパラメータの範囲を設定\n",
    "2. 設定された範囲からランダムにサンプリング3\n",
    "3. 1. でサンプリングされたパラメータを利用して、エポックを小さめにした検証データで認識精度を評価する\n",
    "4. 1.2.をある程度(100回とか)繰り返して、認識精度の結果からパラメータの範囲を狭め得る\n",
    "\n",
    "## まとめ\n",
    "\n",
    "- パラメータの更新方法SGD/Momentum. AdaGrad, Adamを学んだ\n",
    "- 重みの初期値として、Xavierの初期値やHeの初期値(ReLU専用)等が有効\n",
    "- Batch Normalizationを用いることで、学習を早くすすめることができる\n",
    "- 過学習を抑制するための正則化の技術として、weight decayやdropoutがある\n",
    "- ハイパーパラメータは範囲をだんだん狭めながらいいは安易を探す"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
